{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matemática en torno al aprendizaje automático es fascinante, y nos alegramos de que hayas hecho clic en el vínculo para obtener más información. Sin embargo, ten en cuenta que TensorFlow se ocupa de todos los cómputos de gradientes por ti, de manera que no es necesario que comprendas el cálculo que se incluye aquí.\n",
    "\n",
    "### Derivadas parciales\n",
    "\n",
    "Una *función multivariable* es una función con más de un argumento numérico, como la siguiente:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "f(x, y) = e^{2y}sin(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "La **derivada parcial de f con respecto a x**, denotada de la siguiente manera:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{f}}{\\partial{x}}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "es la derivada de f considerada como la función de **x** sola. Para obtener lo siguiente:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{f}}{\\partial{x}}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "debes mantener y constante (para que f ahora sea una función de una variable x) y tomar la derivada regular de f con respecto a x. Por ejemplo, cuando y está fija en 1, la función anterior se vuelve:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "f(x) = e^{2}sin(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Esta es solo una función de una variable x, cuya derivada es:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "e^{2}cos(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "En general, al pensar en y como fija, la derivada parcial de f con respecto a x se calcula de la siguiente manera:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{f}}{\\partial{x}}(x, y) = e^{2y}cos(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "De la misma forma, si x es un valor fijo, la derivada parcial de f con respecto a y es:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{f}}{\\partial{y}}(x, y) = 2e^{2y}sin(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "De manera intuitiva, una derivada parcial te indica cuánto cambia la función cuando se perturba algo una variable. En el ejemplo anterior:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial{f}}{\\partial{y}}(0, 1) = e^2 \\approx 7.4\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Cuando comienzas en (0,1), mantienes y constante y mueves x levemente, f cambia alrededor de 7.4 veces la cantidad que cambiaste x.\n",
    "\n",
    "En el aprendizaje automático, las derivadas parciales se usan mayormente en conjunto con la gradiente de una función.\n",
    "\n",
    "### Gradientes\n",
    "\n",
    "La **gradiente** de una función, denotada de la siguiente manera, es el vector de las derivadas parciales con respecto a todas las variab les independientes:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\nabla{f}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Por ejemplo, si:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "f(x, y) = e^{2y}sin(x)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "entonces:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\nabla{f}(x, y) = \\frac{\\partial{f}}{\\partial{x}}(x, y), \\frac{\\partial{f}}{\\partial{y}}(x, y) = (e^{2y}cos(x), 2e^{2y}sin(x))\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Ten en cuenta lo siguiente:\n",
    "\n",
    "* $\\nabla{f}$: Puntos en la dirección del mayor aumento de la función.\n",
    "* $-\\nabla{f}$: Puntos en la dirección de la mayor disminución de la función.\n",
    "\n",
    "El número de dimensiones de este vector es igual a la cantidad de variables en la fórmula para f; en otras palabras, \"reside\" en el espacio del dominio de la función. Por ejemplo, el gráfico de la siguiente función f (x, y):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "f(x, y) = 4+(x-2)^2+2y^2\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "cuando se ve en tres dimensiones con z = f (x, y) parece un valle con un mínimo en (2,0,4):\n",
    "\n",
    "![graph](img/ThreeDimensionalPlot.svg \"graph\")\n",
    "\n",
    "El gradiente de f (x, y) es un vector de dos dimensiones que te indica en qué dirección (x, y) moverse para obtener la máxima reducción de altura. En otras palabras, el vector de gradiente apunta hacia el valle.\n",
    "\n",
    "En el aprendizaje automático, las gradientes se usan en el descenso de gradientes. Con frecuencia tenemos una función de pérdida de muchas variables que intentamos minimizar, y tratamos de hacerlo al seguir el negativo de la gradiente de la función."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
